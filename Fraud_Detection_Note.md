# Fraud Detection Project

Within the Top-button learning method, I am going to cover the parts in
1. [Knowing the big picture](#1-knowing-the-big-picture)
2. [Learn from the codes (via debugging)](#2-learn-from-the-codes)
3. [Use different datasets to practice](#3-use-different-datasets-to-practice) 
4. [Summary and takeaways](#4-summary-and-takeaways)

## 1. Knowing the big picture

### 1.1 Overview of Fraud Detection
**Technicle skills**
- xgboost
- logistic regression
- SMOTE

**Libraries**
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- imbalanced-learn

**Why are we learning those skills via fraud detection?**
- practical and measurable
- class imbalance problem
- feature importance
- Cost-Sensitive Learning

## 2. Learn from the codes
The codes are generated by Claude 3.7 sonnet agent mode
with the dataset from kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

- use functional programming
- `laod_and_clean_data`
- `perform_eda`
- `prepare_data`
- `handle_imbalanced_data`
- `train_and_evaluate_model`
- `main`

- set up the random seed globally

### 2.1 `main`
|Steps| Usage | 
|---|---|
| 1. print the statement|good habit for clarifying the code| |
| 2. load and clean data| calling `load_and_clean_data`| 
| 3. perform EDA| calling `perform_eda`| 
| 4. prepare data| calling `prepare_data`| 
| 5. handle imbalanced data| calling `handle_imbalanced_data`| 
| 6. train and evaluate model| calling `train_and_evaluate_model` <br> - logistic regression  <br> - xgboost|
| 7. compare the models| use `f1_score` and visualize the result|


### 2.2 `load_and_clean_data`
arguments: `file_path`
return: `df`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. print the statement|print loading and cleaning message||
|2. load the data|use `pd.read_csv` to load the data||
|3. check for missing values|- define `missing_values` with `df.isnull().sum()` <br> - check if there are missing values: <br> &nbsp;&nbsp;- yes: `df.dropna()` to drop missing values <br> &nbsp;&nbsp;- no: print the statement|- `df.isnull()` series with boolean values <br> - `df.isnull().sum()` series with the number|
|4. check for duplicates|- define `duplicates` with `df.duplicated().sum()` <br> - check if there are duplicates: <br> &nbsp;&nbsp;- yes: `df.drop_duplicates()` to drop duplicates <br> &nbsp;&nbsp;- no: print the statement||
|5. print the shape of the dataset|use `df.shape` (rows, columns)||


### 2.3 `perform_eda`
arguments: `df`
return: `df`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Print the statement|Print message indicating EDA is starting||
|2. Basic statistics|Use `df.describe()` to get basic statistics||
|3. Class distribution|- Get class distribution with `df['Class'].value_counts()` <br> - Calculate fraud percentage: `class_distribution[1] / len(df) * 100`|`value_counts()` returns series: <br> `0    284315` <br> `1       492` <br> `dtype: int64`|
|4. Visualize class distribution|- Set figure size with `plt.figure()` <br> - Create countplot with `sns.countplot()` <br> - Save figure with `plt.savefig()`||
|5. Feature correlation|- Get correlations with `df.corr()['Class'].sort_values(ascending=False)` <br> - Shows correlation between Class and other features|`df.corr()` creates correlation matrix dataframe|
|6. Visualize correlations|- Get correlation matrix with `df.corr()` <br> - Create mask with `np.triu()` <br> - Plot heatmap with `sns.heatmap()`|- `np.triu()` gets upper triangle <br> - Mask avoids redundancy <br> - Heatmap params: <br> &nbsp;&nbsp;- `annot=False` <br> &nbsp;&nbsp;- `cmap='coolwarm'` <br> &nbsp;&nbsp;- `linewidths=0.5`|
|7. Amount distribution|- Create subplot with `plt.subplot()` <br> - Plot histogram with `sns.histplot(df['Amount'])` <br> - Plot boxplot with `sns.boxplot()`|`plt.subplot(row, col, index)` sets up subplot layout|
|8. Time distribution|- Create subplot with `plt.subplot()` <br> - Plot histogram with `sns.histplot(df['Time'])` <br> - Plot boxplot with `sns.boxplot()`||
|9. Print completion|Print message indicating EDA is completed||

**Why do we need to have the distribution of the amount and time?**
- To get if imbalanced
- We only have the time and amount features in the dataset
Amount distribution shows:
- Normal transaction amounts
- Outliers
- Fraud vs normal patterns
- Need for data transforms

Time distribution shows:
- Peak times
- Unusual patterns  
- When fraud happens
- Time feature usefulness
  
### 2.4 `prepare_data`
arguments: `df`
return: `X_train`, `X_test`, `y_train`, `y_test`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Print preparation message|Print statement that data preparation is starting||
|2. Separate features and target|- Create features X: `df.drop('Class', axis=1)` <br> - Create target y: `df['Class']`|`axis=1` drops column instead of row|
|3. Split the data|- Use `train_test_split()` to create: <br> &nbsp;&nbsp;- `X_train`, `X_test` <br> &nbsp;&nbsp;- `y_train`, `y_test`|Parameters: <br> - `test_size=0.2` <br> - `random_state=42` <br> - `stratify=y`|
|4. Print shapes|Print training and testing set shapes|Shows data split sizes|
|5. Return the sets|Return the sets: `X_train`, `X_test`, `y_train`, `y_test`||

### 2.5 `handle_imbalanced_data`
arguments: `X_train`, `y_train`
return: `X_train`, `X_test`, `y_train`, `y_test`

1. Print statement: to present we are handling imbalanced data
2. Print class distribution before resampling
    - use `pd.Series(y_train).value_counts()` to get the distribution
    - for confirming the imbalanced data of y_train
    - extra note:
        - `pd.Series()` converts the series to a pandas series
        - `value_counts()` returns the count of each unique value in the series
3. Create pipeline with SMOTE and RandomUnderSampler
5. Print class distribution after resampling
6. Return the sets

### 2.6 `train_and_evaluate_model`





## 3. Use different datasets to practice
