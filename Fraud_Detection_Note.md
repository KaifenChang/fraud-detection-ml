# Fraud Detection Project

Within the Top-button learning method, I am going to cover the parts in
1. [Knowing the big picture](#1-knowing-the-big-picture)
2. [Learn from the codes (via debugging)](#2-learn-from-the-codes)
3. [Use different datasets to practice](#3-use-different-datasets-to-practice) 
4. [Summary and takeaways](#4-summary-and-takeaways)

***
## 1. Knowing the big picture
***
### 1.1 Overview of Fraud Detection
**Technicle skills**
- xgboost
- logistic regression
- SMOTE

**Libraries**
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- imbalanced-learn

**Why are we learning those skills via fraud detection?**
- practical and measurable
- class imbalance problem
- feature importance
- Cost-Sensitive Learning

***
## 2. Learn from the codes
***
The codes are generated by Claude 3.7 sonnet agent mode
with the dataset from kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud

- use functional programming
- `laod_and_clean_data`
- `perform_eda`
- `prepare_data`
- `handle_imbalanced_data`
- `train_and_evaluate_model`
- `main`

- set up the random seed globally

### 2.1 `main`
1. Print the statement - good habit for clarifying the code
- code: `print("Credit Card Fraud Detection")`
2. Load and clean data - calling `load_and_clean_data`
- code: `df = load_and_clean_data('data/creditcard.csv')`
3. Perform EDA - calling `perform_eda`
- code: `perform_eda(df)`
4. Prepare data - calling `prepare_data`
- code: `X_train, X_test, y_train, y_test = prepare_data(df)`
5. Handle imbalanced data - calling `handle_imbalanced_data`
- code: `X_resampled, y_resampled = handle_imbalanced_data(X_train, y_train)`
6. Train and evaluate model - calling `train_and_evaluate_model`
- code: `lr_model, lr_pred, lr_prob = train_and_evaluate_model(X_resampled, X_test, y_resampled, y_test, "Logistic Regression", lr_model)`
   - Logistic regression
   - XGBoost
7. Compare the models - use `f1_score` and visualize the result
- code: `compare_models(models, X_test, y_test)`    

### 2.2 `load_and_clean_data`
arguments: `file_path`
return: `df`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. print the statement|print loading and cleaning message||
|2. load the data|use `pd.read_csv` to load the data||
|3. check for missing values|- define `missing_values` with `df.isnull().sum()` <br> - check if there are missing values: <br> &nbsp;&nbsp;- yes: `df.dropna()` to drop missing values <br> &nbsp;&nbsp;- no: print the statement|- `df.isnull()` series with boolean values <br> - `df.isnull().sum()` series with the number|
|4. check for duplicates|- define `duplicates` with `df.duplicated().sum()` <br> - check if there are duplicates: <br> &nbsp;&nbsp;- yes: `df.drop_duplicates()` to drop duplicates <br> &nbsp;&nbsp;- no: print the statement||
|5. print the shape of the dataset|use `df.shape` (rows, columns)||


### 2.3 `perform_eda`
arguments: `df`
return: `df`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Print the statement|Print message indicating EDA is starting||
|2. Basic statistics|Use `df.describe()` to get basic statistics||
|3. Class distribution|- Get class distribution with `df['Class'].value_counts()` <br> - Calculate fraud percentage: `class_distribution[1] / len(df) * 100`|`value_counts()` returns series: <br> `0    284315` <br> `1       492` <br> `dtype: int64`|
|4. Visualize class distribution|- Set figure size with `plt.figure()` <br> - Create countplot with `sns.countplot()` <br> - Save figure with `plt.savefig()`||
|5. Feature correlation|- Get correlations with `df.corr()['Class'].sort_values(ascending=False)` <br> - Shows correlation between Class and other features|`df.corr()` creates correlation matrix dataframe|
|6. Visualize correlations|- Get correlation matrix with `df.corr()` <br> - Create mask with `np.triu()` <br> - Plot heatmap with `sns.heatmap()`|- `np.triu()` gets upper triangle <br> - Mask avoids redundancy <br> - Heatmap params: <br> &nbsp;&nbsp;- `annot=False` <br> &nbsp;&nbsp;- `cmap='coolwarm'` <br> &nbsp;&nbsp;- `linewidths=0.5`|
|7. Amount distribution|- Create subplot with `plt.subplot()` <br> - Plot histogram with `sns.histplot(df['Amount'])` <br> - Plot boxplot with `sns.boxplot()`|`plt.subplot(row, col, index)` sets up subplot layout|
|8. Time distribution|- Create subplot with `plt.subplot()` <br> - Plot histogram with `sns.histplot(df['Time'])` <br> - Plot boxplot with `sns.boxplot()`||
|9. Print completion|Print message indicating EDA is completed||

**Why do we need to have the distribution of the amount and time?**
- To get if imbalanced
- We only have the time and amount features in the dataset
Amount distribution shows:
- Normal transaction amounts
- Outliers
- Fraud vs normal patterns
- Need for data transforms

Time distribution shows:
- Peak times
- Unusual patterns  
- When fraud happens
- Time feature usefulness
  
### 2.4 `prepare_data`
arguments: `df`
return: `X_train`, `X_test`, `y_train`, `y_test`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Print preparation message|Print statement that data preparation is starting||
|2. Separate features and target|- Create features X: `df.drop('Class', axis=1)` <br> - Create target y: `df['Class']`|`axis=1` drops column instead of row|
|3. Split the data|- Use `train_test_split()` to create: <br> &nbsp;&nbsp;- `X_train`, `X_test` <br> &nbsp;&nbsp;- `y_train`, `y_test`|Parameters: <br> - `test_size=0.2` <br> - `random_state=42` <br> - `stratify=y`|
|4. Print shapes|Print training and testing set shapes|Shows data split sizes|
|5. Return the sets|Return the sets: `X_train`, `X_test`, `y_train`, `y_test`||

### 2.5 `handle_imbalanced_data`
arguments: `X_train`, `y_train`
return:  Resampled `X_train`, `y_train`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Print statement|Print message indicating imbalanced data handling is starting||
|2. Print class distribution before resampling|Use `pd.Series(y_train).value_counts()` to get distribution|- `pd.Series()` converts to pandas series <br> - `value_counts()` counts unique values <br> - Confirms imbalance in y_train|
|3. Create pipeline with SMOTE and RandomUnderSampler|- Create `over` with SMOTE <br> - Create `under` with RandomUnderSampler <br> - Create `steps` list of tuples <br> - Create pipeline with `Pipeline(steps)`|- SMOTE generates minority samples <br> - RandomUnderSampler reduces majority <br> - Steps contain name and estimator pairs|
|4. Apply resampling|Use `pipeline.fit_resample(X_train, y_train)` to resample|- Handles imbalanced data <br> - Returns X_resampled, y_resampled|
|5. Print class distribution after resampling|Check distribution of resampled target y|Verifies data is now balanced|
|6. Return sets|Return `X_resampled`, `y_resampled`||

### 2.6 `train_and_evaluate_model`
arguments: `X_train`, `X_test`, `y_train`, `y_test`, `model_name`, `model`
return: `model`, `predictions`, `probabilities`

|Steps|Actions|Extra Notes|
|---|---|---|
|1. Define function and arguments|Print statement indicating start of training||
|2. Model training|- Set up timer <br> - Train model with `model.fit(datasets)` <br> - End timer|`time.time()` gets current time|
|3. Prediction and Probability|- Make predictions <br> - Get prediction probabilities <br> - Check for `predict_proba` <br> - Use `model.predict(X_test)` if no probabilities|- `y_pred = model.predict(X_test)` <br> - `y_prob = model.predict_proba(X_test)[:, 1]` <br> &nbsp;&nbsp;- Gets probability for class 1|
|4. Model evaluation|- Print classification report|- `print(f"\n{model_name} Evaluation:")` <br> - `classification_report()`: prints precision, recall, f1-score, support <br> - `confusion_matrix()`: prints confusion matrix|
|5. Calculate metrics and visualize|- Calculate metrics: <br> &nbsp;&nbsp;- precision_score() <br> &nbsp;&nbsp;- recall_score() <br> &nbsp;&nbsp;- f1_score() <br> - Plot precision-recall curve|- `: .4f` formats to 4 decimal places <br> - `pr_auc = auc(recall_curve, precision_curve)`|
|6. Return results|Return model, predictions, and probabilities||


## 3. Use different datasets to practice
